HSF-CWP-2017-14

Software trigger and event reconstruction

Editors : David Lange, Princeton University (David.Lange@cern.ch), Vladimir Vava Gligorov, LPNHE, Universite Pierre et Marie Curie, Universite Paris Diderot, CNRS/IN2P3 (vgligoro@lpnhe.in2p3.fr) 

Contributors [incomplete, please add] : Johannes Albrecht, Ken Bloom, Tommaso Boccali, Antonio Boveia (Ohio State University), Michel De Cian (University of Heidelberg), Caterina Doglioni (Lund University), Agnieszka Dziurda (CERN), Markus Elsing, Amir Farbin, Conor Fitzpatrick (EPFL), Frank Gaede (DESY), Simon George, Hadrien Grasland, Lucia Grillo (University of Manchester), Benedikt Hegner (CERN), William Kalderon (Lund University), Sami Kama, Thorsten Kollegger, Patrick Koppenburg (Nikhef), Slava Krutelyov, Rob Kutschke (Fermilab), Walter Lampl, Ed Moyse, Andrew Norman, Marko Petric, Francesco Polci (LPNHE), Karolos Potamianos, Gerhard Raven, Fedor Ratnikov (HSE, YSDA), Martin Ritter,  Andrea Rizzi, David Rousseau, Andy Salzburger, Liz Sexton Kennedy(Fermilab), Michael D Sokoloff (University of Cincinnati) Sokoloff, Graeme Stewart (CERN), Mohammad Al-Turany,  Andrey Ustyuzhanin (HSE, YSDA), Brett Viren, Mike Williams (MIT), Frank Winklmeier (University of Oregon), Frank Wuerthwein (University of California, San Diego), Eduardo Rodrigues (University of Cincinnati)


\section{Introduction and Scope}

Realizing the physics programs of the planned and/or upgraded high-energy physics (HEP) experiments over the next 10 years will require the HEP community to address a number of challenges in the area of software and computing. For this reason, the HEP software community has engaged in a planning process over the past two years, with the objective of identifying and prioritizing the research and development required to enable the next generation of HEP detectors to fulfill their full physics potential. The aim is to produce a Community White Paper (CWP) [HSF2017] which will describe the community strategy and a roadmap for software and computing research and development in HEP for the 2020s. This activity is organised under the umbrella of the HEP software foundation (HSF). The LHC experiments and HSF have been specifically charged by the WLCG project, but have reached out to other HEP experiments around the world throughout the community process in order to make it as representative as possible.

The CWP process was carried out by working groups centered on specific topics. The topics of event reconstruction and software triggers are covered together in this document and have resulted from discussions within a single working group. The reconstruction of raw detector data and simulated data and its processing in real time represent a major component of today's computing requirements in high-energy physics. A recent projection [Campana2016] of the ATLAS 2016 computing model results in >85\% of the HL-LHC CPU resources being spent on the reconstruction of data or simulated events. This working group evaluated the most important components of next generation algorithms, data structures, and code development and management paradigms needed to cope with highly complex environments expected in high-energy physics detector operations in the next decade. New approaches to data processing were also considered, including the use of novel, or at least, novel to HEP, algorithms, and the movement of data analysis tasks into real-time environments. 

The remainder of this document is organized as follows. First we discuss how future changes including new and proposed facilities, detector designs, and evolutions in computing and software technologies change the requirements on software trigger and reconstruction applications. Second, we summarize current practices and identify the most limiting components in terms of both physics and computational performance. Finally we propose a research and development roadmap for the software trigger and event reconstruction areas including a survey of relevant on-going work for the topics identified in the roadmap. 

Of course any discussion of the computing challenges and priorities for software triggers and reconstruction necessarily overlaps with software domains covered by other CWP documents. Indeed, the critical role of real-time reconstruction in allowing the LHC data to be collected in the first place means that the requirements set out here will drive much of the R\&D across other areas, whether that be in the development of more performant math libraries, simplified but accurate detector descriptions, or new reconstruction algorithms based on machine learning paradigms. Such areas of overlap are noted wherever relevant in the text, and the reader is encouraged to refer to the other CWP documents for more details.

\section{Nomenclature}

This document will discuss software algorithms essential to the interpretation of raw detector data into analysis level objects in several contexts. Specifically, these algorithms can be categorized as: 
\begin{enumerate}
\item Online: Algorithms, or sequences of algorithms, executed on events read out from the detector in near-real-time as part of the software trigger, typically on a computing facility located close to the detector itself.
\item Offline: As distinguished from online, any algorithm or sequence of algorithms executed on the subset of events preselected by the trigger system, or generated by a Monte Carlo simulation application, typically in a distributed computing system.
\item
Reconstruction : The transformation of raw detector information into higher level objects used in physics analysis. Depending on the experiment in question, these higher level objects might be charged particle trajectories (“tracks”), neutral or charged particle calorimeter clusters, Cherenkov rings, jets, and so on. A defining characteristic of “reconstruction” which separates it from “analysis” is that the quality criteria used in the reconstruction to, for example, minimize the number of fake tracks, are independent of how those tracks will be used later on. Reconstruction algorithms are also typically run as part of the processing carried out by centralized computing facilities.
\item
Trigger: the online classification of events, performed with the objective of reducing either the number of events which are kept for further “offline” analysis, the size of such events, or both. In this working group we were only concerned with software triggers, whose defining characteristic is that they process data without a fixed latency. Software triggers are part of the real-time processing path and must make processing decisions quickly enough to keep up with the incoming data, possibly with the benefit of substantial disk buffers.
\item
Real-time analysis: The typical goal of a physics analysis is to combine the products of the reconstruction algorithms (tracks, clusters, jets...) into complex objects (hadrons, gauge bosons, new physics candidates...) which can then be used to infer some fundamental properties of nature (CP asymmetry, lepton universality, Higgs couplings...). We define real-time analysis any physics analysis step that goes beyond object reconstruction and is performed online within the trigger system, in certain cases using simplified algorithms to fit within the trigger system constraints. Real-time analysis techniques are so far quite experiment-specific. Techniques may include the selection and classification of all objects crucial to the calibration of the detector performance, evaluation of backgrounds, as well as physics searches that are otherwise impossible given limitations of data samples passing the trigger and saved for offline work.
\end{enumerate}

The online and offline algorithms have traditionally been viewed as related, but at least partly separate due to their differing goals and requirements. Because the online algorithms have to run on all events read out from the detector [FOOTNOTE], they typically must be executed on dedicated computer facilities (e.g. a server farm) located near to the detector in order to avoid prohibitive networking and data transfer costs [FOOTNOTE]. Such dedicated farms typically have a small (if any) amount of disk space to buffer events while waiting for them to be processed, and the online algorithms must therefore be tuned to strict timing and memory consumption requirements. In contrast the offline algorithms run only on a subset of events which have been saved to long term storage. They must still execute within a reasonable time so that their output is made available for analysis in a timely fashion, and to fit the computing resources available to the experiment, but these pressures are generally much less severe than for online processing. In addition, online algorithms often run in dedicated frameworks, with additional layers of safety or control compared to their offline counterparts. 

Increasingly, however, the difficulties of maintaining these parallel software environments is driving online algorithms to become special cases of the offline ones configured for increased speed at the cost of precision, but otherwise relying on the same underlying codebase. This development is also driven by the desire to reduce systematic uncertainties introduced by having separate online and offline reconstruction, in particular for “precision” measurements.

This physics motivation to use offline algorithms online can lead to performance improvements in offline algorithms beyond what might have otherwise been achieved. In turn, such improvements free up offline resources, notably for producing the large samples of simulated events which will be needed in the HL-LHC period. We therefore assume that this trend will continue and intensify on the timescale considered in this document.

\section{New Challenges anticipated on the 5-10 year timescale}

This section summarizes the challenges identified by the working group for software trigger and event reconstruction techniques in the next decade. We have organized these challenges into those from new and upgrade accelerator facilities, from detector upgrades and new detector technologies, increases in anticipated event rates to be processed by algorithms (both online and offline), and from evolutions in software development practices.

\subsection{Challenges posed by Future Facilities }

Here we briefly describe some of the relevant accelerator facility upgrades and their impact on experimental data samples. These will be the basis of our discussion of how software trigger and event reconstruction algorithms must evolve.
LHC Run 3: Run 3 of the LHC is expected to last three years, starting in 2021, after the second long shutdown of the LHC. Having already exceeded the design luminosity of the LHC facility, there is no significant increase in instantaneous luminosity expected for the CMS and ATLAS experiments. The CMS and ATLAS experiments expect to accumulate up to 300 fb-1 of data each by the end of this run [Bordry2016]. This is a nearly 10x increase over the samples of 13 TeV data collected through 2016. Both LHCb and ALICE will undergo major upgrades for Run 3 (described in the next section) : LHCb will have an instantaneous luminosity five times higher than in Run 2 [LHCb2012], while ALICE will upgrade [ALICE2013] its readout and real-time data processing in order to enable the full 50 kHz Pb-Pb collision rate to be saved for offline analysis.
High-luminosity LHC (HL-LHC): The HL-LHC project [Zimmerman2009] is currently planned to begin operations in 2026. It is an upgrade to LHC, expected to result in an increase of up to a factor of 10 in instantaneous luminosity over the LHC design (so up to 1035 cm2s-1). The beam energy of 14 TeV and 25 ns bunch spacing imply a considerable increase in the number of simultaneous collisions (pileup) seen by experiments. Operating scenarios under study are pileup of 140 or 200 for ATLAS [ATLAS2015] and CMS [CMS2015] at a 25 ns bunch spacing, both possibly with luminosity leveling techniques that would provide a relatively constant luminosity throughout a fill. In addition, LHCb [LHCb2017] is planning a consolidation during LS3 for initial HL-LHC operations, with improvements to various detector components, followed by a potential later Upgrade II with a pileup of 50 to 60, to run from roughly 2030 onwards. The HL-LHC project is expected to run for at least 10 years. 
Super KEKB: The Super KEKB facility [Ohnishi2013], together with the Belle-II experiment [BelleII2010], plans to achieve a 40 times increase in instantaneous luminosity over that achieved by the previous generation of e+e- colliders operating on or near the Upsilon(4S) resonance (KEK-B and PEP-II). Super KEKB should begin production data taking in 2018 and run until at least 2024 with a goal of 50 ab-1 of data collected (e.g., nearly 60 billion B-B pair events recorded). 
Long Baseline Neutrino Facility (LBNF): The LBNF project [DUNE2016] is planning a high-intensity neutrino beamline from Fermilab to the SURF underground facility in South Dakota. Based on the NuMI beamline, LBNF plans a proton-beam power of 1.2 MW (7.5x1013 protons per cycle) later upgraded to 2.4 MW (1.5-2.0x1014 protons per cycle). The facility expects to operate for 20 years starting 2025.
Linear Colliders (ILC and CLIC): two electron-positron collider projects are currently under study, the International Linear Collider (ILC), to be built in Japan and the Compact Linear Collider (CLIC) at CERN. The ILC will operate at a center of mass energy of 250-500 GeV. With a nominal luminosity of 1.47⨉ 1034 cm-2s-1 at 500 GeV and an expected raw data rate of 1 GB/s the two planned experiments will each accumulate up to 10 PB/year. Both colliders plan to run without any hardware trigger, requiring a fast and efficient prompt reconstruction and event building.
Future Circular Collider (FCC): A 100 TeV facility is being studied as the next step in the energy frontier projects after HL-LHC [Ball2014]. This could be realized as a 100 km circumference tunnel using 16 T magnets. Scenarios under discussion for such a facility include up to 1000 pileup events and the need to be hermetic up to a much larger eta (eg. η=6) than planned for the HL-LHC upgrades to the ATLAS or CMS experiments. 
Several common themes are apparent from these facility plans. Accelerator operating conditions continue to evolve towards higher intensity and higher energy, as required to bring new discovery potential. This means more complex and higher particle density environments from which signs of new physics must be extracted by trigger systems, event reconstruction algorithms and by analysis. This complexity brings new requirements and challenges to detector design as well as software algorithms, where detection efficiency needs to be maintained in more complex environments without increasing false-positive rates. 

For HL-LHC, the increased pileup of many interactions in a single crossing leads to several critical problems. The higher particle multiplicities and detector occupancies will lead to a significant slowdown in all reconstruction algorithms, from the tracking itself to the reconstruction in other devices such as the electromagnetic calorimeter and RICH detectors. In addition to making the algorithms slower, pileup also leads to a loss of physics performance, for example :
\begin{itemize}
\item Reduced reconstruction efficiency in Electromagnetic calorimeters
\item Increased association of tracks to wrong primary vertices
\item Reduced efficiency of identifying isolated electrons, muons, taus, and photons
\item Reduced selection efficiencies for electrons and photons
\item Reconstruction efficiencies for hadronic tau decays and b-jets
\item Worse energy resolution for electrons, photons, taus, jets, and ETmiss
\item Worse reconstruction of jet properties (substructure for top/W-tagging, quark/gluon discrimination, etc)
\end{itemize}

The central challenge for object reconstruction at HL-LHC is thus to maintain excellent efficiency and resolution in the face of high pileup values, especially at low object pT. Detector upgrades such as increases in channel density, high precision timing and improved detector geometric layouts are essential to overcome these problems. For software, particularly for triggering and event reconstruction algorithms, there is an additional need not to dramatically increase the event processing time. A comprehensive program of studies is required to assess the efficiencies and resolutions for various approaches in events with up to 200 pileup interactions. 

The increase in event complexity also brings a “problem” of overabundance of signal to the experiments, and specifically the software trigger algorithms. Traditional HEP triggers select a small subset of interesting events and store all information recorded in such events. This approach assumes first that only a very small fraction of collisions potentially contain interesting physics, second that the features of such events will be strikingly different than the features of “uninteresting” events, and third that discarding any information in an event would prevent later correcting any defects in the real-time processing, or preventing a full understanding of the process of interest. The evolution towards a genuine real-time processing of data has been driven by a breakdown in the first two assumptions and technological developments, which means that the third assumption is no longer as worrying as it once was.

An illustrative example is the search for low-mass dark matter at the LHC, such as the  search for dark photons at LHCb [Ilten2016].  Since interactions between dark photons and Standard Model (SM) particles have very low cross sections, the probability of producing dark photons in proton-proton collisions is extremely small. Thus, discovering them at the LHC will require an immense number of proton-proton collisions and a highly efficient trigger. The key problem is that when the dark photon lifetime is small compared to the detector resolution, which is the case in much of the interesting parameter space, there is an overwhelming irreducible SM background from off-shell photons producing di-muon events. The current LHCb trigger configuration discards about 90\% of potential dark photon decays, and a variety of other beyond the Standard Model and SM signals where the LHCb detector itself has good sensitivity. Once this stage is removed (and the luminosity is increased), the potential signal rate will increase by a factor of 50. However, the irreducible SM-background rate also increase significantly. Since offline storage resources are not expected to increase at nearly this rate, the data must be compressed much more than is now within the online system. In other words, techniques developed in the offline analysis for background must be integrated into the online software trigger. 

Similar considerations apply to low-mass dijet searches at ATLAS/CMS, where the enormous background rate from quantum chromodynamics (QCD), which grows linearly with pileup, limits the study of physics at the electroweak scale in hadronic final states given current techniques. Trigger output bandwidth limitations mean that only a tiny fraction of lower-energy jets can be recorded, and hence the potential statistical precision of searches involving these low energy jets is vastly reduced. This is again relevant in the context of dark matter searches, here for light mediators between quarks and dark matter particles with low coupling strength. Further details are described in [aTLAs2017, CMS2016].

These considerations also apply to other areas of the LHC physics program. For example, by Run 3 [LHCb2014], most LHC bunch crossings will produce charm hadrons at least partially in the LHCb acceptance, and all bunch crossings will produce strange hadrons detectable in the LHCb acceptance. Even more dramatically, in the HL-LHC era, a potential Upgrade II of LHCb will have to cope with multiple reconstructible charm hadron signals per bunch crossing, and even semileptonic B decays will become more abundant than can be stored offline. The ability of the LHC to continue improving our knowledge in these areas will therefore entirely depend on the ability to select the specific signal decay modes in question at the trigger level. Taken together, the overabundance of interesting signals, which mandates more complex event reconstruction at the trigger level, and the increasing event complexity, which makes the event reconstruction ever more expensive, will influence the design and requirements of trigger and reconstruction algorithms over the next decade.

\subsection{Challenges and Opportunities from Evolution in Experimental apparatus}

A number of new detector and hardware trigger concepts are proposed on the 5-10 year timescale in order to help in overcoming the challenges identified above. In many cases, these new technologies bring novel requirements to software trigger and event reconstruction algorithms. These include:

High-granularity calorimetry: Experiments including CMS (for HL-LHC), ILC, CLIC and FCC are proposing very high granularity calorimeters in order to better separate showers from closeby particles in a high-density (i.e., high-pileup) environment and thereby improve the jet energy resolution. This granularity brings significant computational challenges as there is much more information to process in order to fully take advantage of these devices. Efficient algorithms [Marshall2015,Gray2016] are needed to fully optimize the ambiguity reduction capability of the signals from millions of channels within finite computing times. 

Precision timing detectors [Gray2017] for charged particles: Experiments including ATLAS, CMS and LHCb are pursuing timing detectors with precision approaching 30 ps. This information is another tool for dramatically reducing the effect of pileup on triggering and reconstruction algorithms in particular in the areas of tracking and vertex finding. Integrating timing information into Kalman filtering and vertex disambiguation algorithms can improve both physics performance and time-to-process events online and offline. Making current tracking plus new timing detectors effectively integrate into a performant 4D-detection system will necessitate the development of new reconstruction algorithms that make use of both spatial and timing information. 

Hardware triggers based on tracking information: Hardware trigger systems designed to identify tracks down to 2 GeV of pT in the are a valuable tool for ATLAS and CMS [ATLAS2015,CMS2015]. This technology will enhance the ability to trigger on a range of physics signatures including isolated leptons, multi-jet signatures and displaced vertices, as well as mitigating the effects of pileup on these objects. Once an event is triggered, the results obtained from these triggers can also be a valuable tool in the software trigger, for example by seeding and therefore speeding up the subsequent reconstruction. Similar systems, with an ability to go below 500 MeV of pT are also under consideration for the LS3 Consolidation and Upgrade II of LHCb, and could be particularly valuable in the study of strange hadrons if the thresholds could be reduced down to 100 MeV of pT.

Data streaming techniques: Experiments with no hardware trigger allow a software trigger to see all data before events are rejected from further processing. There is a clear advantage in physics capability if the data streaming capability is sufficient and if software triggers are efficient, effective and reliable enough for the task. There is always an advantage if additional algorithms can be run on an event before a decision must be taken about its importance. In the case of LHCb this means a facility and algorithms that are capable of processing with a sustained throughput of 30 MHz [LHCb2014]. Similarly, the Alice experiment plans a 50 kHz interaction rate with no, or simple minimum bias, hardware trigger, and will stream 3 TB/second from the TPC in a common online-offline computing system [ALICE2015].

\subsection{Challenges from Event rates and real-time processing}

Trigger systems for next-generation experiments are evolving to be more capable, both in their ability to select a wider range of events of interest for the physics program of their experiment, and their ability to stream a larger rate of events for further processing.  ATLAS and CMS both target systems where the output of the hardware trigger system is increased by 10x over the current capability, up to 1 MHz [ATLAS2015,CMS2015]. In other cases, such as LHCb, the full collision rate (between 30 to 40 MHz for typical LHC operations) will be streamed to real-time or quasi-realtime software trigger systems. It is interesting to note that because the ATLAS/CMS events are O(10) times larger than those of LHCb (roughly O(1) vs O(0.1) MB/event), the resulting data rates are rather similar, namely 1-5 TB/second. 

This enhanced capability naturally increases the demands on software trigger algorithms and offline reconstruction algorithms. In many cases, the current trigger bandwidth of experiments is limited by the offline processing and storage capabilities of an experiment rather than its ability to physically write out data to disk or tape for further processing.  This can be due either to the time to fully reconstruct events for analysis (corresponding to a fixed set of CPU resources)  or by the size of the analysis data itself (corresponding to a fixed set of disk resources). Current experiments are therefore constantly working to reduce the CPU needs to reconstruct events and storage needs for analyzing them, through a combination of code improvements (refactoring, vectorization, exploitation of optimized instruction sets) or through entirely rewriting algorithms. 

This is an ongoing process that continues to yield throughput improvements, aided by improved code analysis tools, modern compilers and other tools. For many experiments, there is also a potential tradeoff between physics quality and CPU needs. A typical example is that track reconstruction requires less CPU if the pT threshold for tracks to be identified is raised, thereby reducing the overall combinatorics. This sort of CPU improvement is almost never desirable as it reduces the overall physics output. Instead software trigger and reconstruction applications are pressed to include more and more algorithms over time. Typical examples for ATLAS and CMS are new jet-finding, isolation, or pileup subtraction approaches, and, more generally, algorithms developed targeting specific physics use cases, for example in the trigger-level search for low-mass hadronic resonances in ATLAS described in [Abreu2014]. Conversely, the need to reconstruct higher multiplicity, or increasingly soft, decays challenges LHCb’s applications.

Recent examples of significant storage reductions for the analysis data tier include the CMS MiniAOD [Petrucciani2015] and the ATLAS xAOD [Eifert2015], both deployed for LHC Run 2 analysis. Improvements can be achieved by refinements in physics object selection (e.g., saving less uninteresting information), and by enhanced compression techniques (using lossless or lossy methods). 

Real-time analysis is the only solution for those signals which are so abundant that they cannot all be saved to disk, or for which discriminating against backgrounds requires the best possible detector calibration, alignment, reconstruction, and analysis. In this case, some or all of the output of the software trigger system also serves as the final analysis format. Its development is justified by the need to conduct the broadest possible program of physics measurements with our existing detectors. This is critical for two reasons: first, because we do not want to miss any signatures of New Physics around the electroweak scale, whether direct or indirect, but also because, even if the New Physics scale lies beyond the reach of current detectors, we must probe the widest possible parameter space in order to motivate and guide the design of future colliders and experiments. This is particularly true given the cost and long timescale of such future facilities.

An early version of this approach consisted of keeping only a limited set of physics objects (e.g., jets) as computed in real-time processing, and proof-of-concept implementations exist since LHC Run 1 [Aaij2016,Abreu2014,CMS2016]. In order to perform precision measurements in real-time, however, it is critical to be able to keep data long enough (hours or days, depending on the experiment) to perform quasi-real-time calibrations and a final offline analysis quality reconstruction in the trigger system itself. This approach was commissioned by LHCb in 2015. As a result, roughly one third of the LHCb experiment’s Run II trigger selections now use the real-time reconstruction to reduce the amount of data kept for further analysis. Advantages of these approaches include an order of magnitude reduction in data volume from not saving the raw detector data, potentially reduced systematic errors in analysis due to differences in algorithms or calibrations used in the online and offline processing, and a reduction or elimination of the need for offline processing. 

High data rates made possible by reducing event size and by processing online abundant data exposes the problem with Monte Carlo (MC) simulation support for those data. MC production is the primary consumer of computing resources. Luminosity increase will proportionally increase the amount of necessary MC samples, especially for pileup contribution in forward regions of calorimeters. The full range GEANT simulation of these dense events will require corresponding increase of computing resources. The alternative solution is based on the fact that actual readout granularity of calorimeters and other detectors, which are heavy for simulation, is significantly lower than the spatial precision of detailed GEANT simulation. This gives an opportunity to substitute detailed GEANT simulation with much faster ML-based generative models, generating detector response with just right topology and granularity.

These advantages allow an entire category of physics to be probed by HL-LHC experiments that would not otherwise be considered. Among the challenges posed by these approaches are the need for very robust event reconstruction algorithms, the need to derive detector calibrations sufficient for final analysis within this short processing window, and the need to plan analyses sufficiently in advance of data taking so that the choices made in the real-time analysis will be robust against eventual systematics studies.

\subsection{Challenges from Evolutions in Computing technology}

This section summarizes recent, and expected, evolutions in computing technologies. These are both opportunities to move beyond commodity x86 technologies, which HEP has used very effectively over the past 20 years, and significant challenges to continue to derive sufficient event processing throughput per cost to enable our physics programs at reasonable computing cost.  A full description of this technology evolution and its effect on HEP is beyond the scope of this document [Bird2014]. Here we identify the main technology changes identified as driving out research and development in the area of software trigger and event reconstruction:
\begin{itemize}
\item
Increase of SIMD capabilities: The size of vector units on modern commodity processors are increasing rapidly. While not all algorithms can easily be adapted to benefit from this capability, large gains are possible where algorithms can be vectorized. Essentially all HEP codes need modifications, or large scale refactoring, to effectively utilize SIMD capabilities.
\item
Evolution towards multi- or many-core architectures: The current trend is to move away from ever faster processing cores towards more power efficient and more numerous processing cores. This change has already broken the traditional “one-core-one-event” model in many HEP frameworks and algorithms. As core counts increase, a larger number of algorithm developers, instead of just those developing the most resource intensive algorithms, will need to incorporate parallelism techniques into their algorithm implementations.   
\item
Slow increase in memory bandwidth: Software trigger and event reconstruction applications in HEP are very memory intensive, and I/O access to memory in commodity hardware has not kept up with CPU capabilities. To evolve towards modern architectures, including the effective use of hierarchical memory structures, HEP algorithms will need to be refactored or rewritten to considerably reduce the required memory per processing core. 
\item
Rise of heterogeneous hardware: Evolution in HEP algorithms has long taken advantage of a single dominant commodity computing platform (x86 running Linux).  More and more studies have shown or are investigating the throughput benefits of using low-power systems, GPUs, FPGA systems for critical pieces of processing. Software trigger and event processing algorithms are those that may benefit the most in HEP from the effective use of these technologies if they are able to adapt to the requirements of using these systems effectively.
\item
Possible evolution in facilities: HEP facilities are likely to evolve both due to the increased architectural variability of affordable hardware and due to evolution in data science techniques. One example of the latter is an analysis center whose design is driven by data science techniques and technologies. These technologies (e.g., Hadoop, Spark) are under investigation in HEP for analysis and may change the way HEP data centers are resourced. A particular example of how this will impact trigger or reconstruction algorithms is that of physics object identification algorithms. These are frequently rerun by analysts in order to include the most recent version developed by the collaboration in their analysis.
\end{itemize}
Evolution in computing technology is generally a slow but continual process. However, architectures available today can provide necessary development platforms for trigger and event reconstruction developers to adapt codes to be better suited to future architectures.

\subsection{Challenges from Evolutions in Software technology}

The status and evolution of software development in HEP is the subject of the Software Development CWP working group. In this section, we briefly discuss some of the issues and opportunities of particular relevance to software trigger and event reconstruction work.

The move towards open source software development and continuous integration systems brings a number of important opportunities to assist developers of software trigger and event reconstruction algorithms. Continuous integration systems have already brought the ability to automate code quality and performance checks, both for algorithm developers and code integration teams. Scaling these up to allow for sufficiently high statistics checks is among the still outstanding challenges. While it is straightforward to test changes where no regression is expected, fully developed infrastructure for assisting developers in confirming the physics and technical performance of their algorithms during development is a work in progress.

As the timescale for experimental data taking and analysis increases, the issues of legacy code support increase. In particular, it seems unaffordable to imagine rewriting all of the software developed by the LHC experiments during the long shutdown preceding HL-LHC operations. Thus, as the HL-LHC run progresses, much of the code base for software trigger and event reconstruction algorithms will be 15-30 years in age. This implies an increased need for sustainable software development and investment in software education for experimental teams.

Code quality demands increase as traditional offline analysis components migrate into trigger systems, or more generically into algorithms that can only be run once. As described above, this may be due to either the prohibitive cost of rerunning algorithms over large data sets, or due to not having retained sufficient data to rerun algorithms (e.g., not retaining the full raw data). Algorithms in the software trigger and event reconstruction areas are very frequently contributed to by a large community of physicists rather than expert programmers. In many cases, the most sensitive algorithms may be checked and optimized by programming experts. This has so far satisfied the need of reducing the total computing resources needed by experiments, as typically only a few algorithmic components dominate the overall computing need. However, this approach would be currently impossible to carry out across the entire reconstruction code stack. As the complexity (and number) of real-time algorithms increases, the need for training as well as code validation and regression checking increases considerably. 

These challenges are further complicated by a growing diversity of developers in the software trigger and event reconstruction areas. The gap between the basic programming techniques which students learn in their undergraduate courses and the state-of-the-art programming techniques required to fully exploit the power of emerging parallel hardware architectures. Software development methods and programming techniques evolve particularly quickly and often developers are self taught on current techniques. The experiment software environment must facilitate contributions from a range of developers while adjusting to challenges of a more and more complex online and offline software toolkit.

\section{Current approaches}



blah~\cite{HSFWEB}
